{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dataset from {file_path}...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    logger.info(\"Dataset loaded.\")\n",
    "    return data\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset.\n",
    "    \"\"\"\n",
    "    logger.info(\"Handling missing values...\")\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Fill missing values with the mean for numerical columns\n",
    "    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())\n",
    "\n",
    "    # Fill missing values with \"Unknown\" for categorical columns\n",
    "    data[categorical_cols] = data[categorical_cols].fillna('Unknown')\n",
    "\n",
    "    logger.info(\"Missing values handled.\")\n",
    "    return data\n",
    "\n",
    "def split_data(data, target_variable):\n",
    "    \"\"\"\n",
    "    Split the data into features (X) and target variable (y).\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting the data into features and target variable...\")\n",
    "    X = data.drop(columns=[target_variable])\n",
    "    y = data[target_variable]\n",
    "    logger.info(\"Data split complete.\")\n",
    "    return X, y\n",
    "\n",
    "def preprocess_numerical_col(col, preprocessing_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a numerical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        for preprocess_name, preprocess_method in preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='passthrough')\n",
    "            for model_name, (model, model_params) in models.items():\n",
    "                pipeline = Pipeline([\n",
    "                    ('preprocessor', preprocessor),\n",
    "                    ('model', model)\n",
    "                ])\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                score = pipeline.score(X_train, y_train)\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score = score\n",
    "                    best_preprocess = preprocess_name\n",
    "        return col, best_preprocess\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, str(e)\n",
    "\n",
    "\n",
    "def preprocess_categorical_col(col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a categorical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        best_feature_selection = None  # Store the best feature selection method\n",
    "        for preprocess_name, preprocess_method in categorical_preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='drop')\n",
    "            for feature_selection_name, feature_selection_method in feature_selection_methods.items():\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_method, [col])])\n",
    "                for model_name, (model, model_params) in models.items():\n",
    "                    pipeline = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('feature_selection', feature_selector),\n",
    "                        ('model', model)\n",
    "                    ])\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_train, y_train)\n",
    "                    if best_score is None or score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preprocess = preprocess_name\n",
    "                        best_feature_selection = feature_selection_name\n",
    "        return col, best_preprocess, best_feature_selection\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, None, str(e)\n",
    "\n",
    "def process_numerical_cols(X_train, numerical_cols, preprocessing_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess numerical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing numerical columns...\")\n",
    "    numerical_cols_processed = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        numerical_cols_processed = parallel(delayed(preprocess_numerical_col)(col, preprocessing_methods, X_train, y_train, models) for col in tqdm(numerical_cols, desc=\"Numerical Columns Preprocessing\"))\n",
    "    logger.info(\"Numerical columns processing complete.\")\n",
    "    return numerical_cols_processed\n",
    "\n",
    "def process_categorical_cols(X_train, categorical_cols, preprocessing_methods, feature_selection_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess categorical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing categorical columns...\")\n",
    "    categorical_cols_processed = []\n",
    "    categorical_cols_processed_temp = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        categorical_cols_processed = parallel(delayed(preprocess_categorical_col)(\n",
    "            col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models\n",
    "        ) for col in tqdm(categorical_cols, desc=\"Categorical Columns Preprocessing\"))\n",
    "\n",
    "    # Add the processed categorical columns to the list\n",
    "    categorical_cols_processed.extend(cols for cols in categorical_cols_processed_temp if cols not in categorical_cols_processed)\n",
    "\n",
    "    return categorical_cols_processed\n",
    "\n",
    "\n",
    "\n",
    "def perform_grid_search(model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                        preprocessing_techniques):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation to find the best hyperparameters and preprocessing techniques.\n",
    "    \"\"\"\n",
    "    pipeline_count = 0\n",
    "    preprocessing_steps = []\n",
    "\n",
    "    if len(cols) > 0:\n",
    "        pipeline_count += 1\n",
    "        preprocessor = ColumnTransformer(preprocessing_techniques, remainder='passthrough')\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        param_grid = {f'model__{param_name}': param_range for param_name, param_range in model_params.items()}\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=KFold(n_splits=5))\n",
    "                grid_search.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Grid search failed for {model_name}: {e}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "        best_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        pipeline_score = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=KFold(n_splits=5)).mean()\n",
    "\n",
    "        if task_type == \"Regression\":\n",
    "            y_pred = grid_search.predict(X_train)\n",
    "            r2 = r2_score(y_train, y_pred)\n",
    "            test_score = grid_search.score(X_test, y_test)\n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Preprocessing': cols,\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Score': best_score,\n",
    "                'Pipeline Score': pipeline_score,\n",
    "                'Accuracy': r2,\n",
    "                'Test Score': test_score,\n",
    "                'Execution Time (s)': end_time - start_time\n",
    "            }\n",
    "        else:\n",
    "            y_pred = grid_search.predict(X_train)\n",
    "            accuracy = accuracy_score(y_train, y_pred)\n",
    "            test_score = grid_search.score(X_test, y_test)\n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Preprocessing': cols,\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Score': best_score,\n",
    "                'Pipeline Score': pipeline_score,\n",
    "                'Accuracy': accuracy,\n",
    "                'Test Score': test_score,\n",
    "                'Execution Time (s)': end_time - start_time\n",
    "            }\n",
    "\n",
    "        preprocessing_steps = [f\"{step[0]}: {step[1]}\" for step in cols]\n",
    "\n",
    "        return result, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "    else:\n",
    "        return {}, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "\n",
    "def perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation in parallel for each model and column combination.\n",
    "    \"\"\"\n",
    "    pipeline_counts = []\n",
    "    preprocessing_techniques_str = []\n",
    "    results = []\n",
    "\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        total_pipelines_count = len(cols) * len(models)\n",
    "        pipeline_bar = tqdm(total=total_pipelines_count, desc='Optimization Progress', leave=False)\n",
    "        for model_name, (model, model_params) in models.items():\n",
    "            try:\n",
    "                processed_results = parallel(delayed(perform_grid_search)(\n",
    "                    model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                    preprocessing_techniques\n",
    "                ) for cols in tqdm(cols, desc=f\"Model: {model_name}\", total=len(cols), file=sys.stdout))\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Grid search parallel failed for {model_name}: {e}\")\n",
    "\n",
    "            results.extend([result for result, _, _, _ in processed_results])\n",
    "\n",
    "            pipeline_counts.extend([(count, steps, techniques) for _, count, steps, techniques in processed_results])\n",
    "\n",
    "            pipeline_bar.update(len(cols))\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            pipeline_bar.set_postfix({'Pipelines Generated': f\"{sum(count for count, _, _ in pipeline_counts)}/{total_pipelines_count}\"})\n",
    "\n",
    "    preprocessing_techniques_str = '\\n'.join([f\"{step[0]}: {step[1]}\" for step in preprocessing_techniques])\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    results_df['Preprocessing Techniques'] = preprocessing_techniques_str\n",
    "\n",
    "    results_df_sorted = results_df.sort_values(by='Best Score', ascending=False)\n",
    "\n",
    "    return results_df_sorted, pipeline_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Load your dataset\n",
    "    data = load_dataset('d.csv')\n",
    "\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "\n",
    "    # Assume the target variable column name is 'target'\n",
    "    target_variable = 'target'\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X, y = split_data(data, target_variable)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Calculate the unique percentage of the target variable\n",
    "    unique_percentage = y.nunique() / y.shape[0]\n",
    "\n",
    "    # Determine the target variable class based on the unique percentage\n",
    "    if unique_percentage <= 0.05:\n",
    "        target_variable_class = \"Binary\"\n",
    "    elif unique_percentage <= 0.1:\n",
    "        target_variable_class = \"Multi-class\"\n",
    "    else:\n",
    "        target_variable_class = \"Regression\"\n",
    "\n",
    "    # Print the target variable class\n",
    "    logger.info(\"Target Variable Class: %s\", target_variable_class)\n",
    "\n",
    "    # Define task type based on the target variable class\n",
    "    if target_variable_class == \"Binary\":\n",
    "        # Binary classification task\n",
    "        task_type = \"Binary Classification\"\n",
    "        models = {\n",
    "            'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Classifier': (MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "            'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "    elif target_variable_class == \"Multi-class\":\n",
    "        # Multi-class classification task\n",
    "        task_type = \"Multi-class Classification\"\n",
    "        models = {\n",
    "            'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Classifier': (MLPClassifier(max_iter=10000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "            'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "    else:\n",
    "        # Regression task\n",
    "        task_type = \"Regression\"\n",
    "        models = {\n",
    "            'Decision Tree Regressor': (DecisionTreeRegressor(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Regressor': (GradientBoostingRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Regressor': (RandomForestRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Regressor': (MLPRegressor(), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Linear Regression': (LinearRegression(), {'normalize': [True, False]}),\n",
    "            'KNN Regressor': (KNeighborsRegressor(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "\n",
    "    # Define your preprocessing techniques\n",
    "    numerical_preprocessing_methods = {\n",
    "        'SimpleImputer': SimpleImputer(),\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'PolynomialFeatures': PolynomialFeatures(),\n",
    "        'PCA': PCA()\n",
    "    }\n",
    "\n",
    "    categorical_preprocessing_methods = {\n",
    "        'OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
    "        'LabelEncoder': LabelEncoder(),\n",
    "    }\n",
    "\n",
    "    feature_selection_methods = {\n",
    "        'SelectKBest_f_regression': SelectKBest(f_regression),\n",
    "        'SelectKBest_chi2': SelectKBest(chi2)\n",
    "    }\n",
    "\n",
    "    # Handle numerical columns\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    numerical_cols_processed = process_numerical_cols(X_train, numerical_cols, numerical_preprocessing_methods, models, y_train)\n",
    "\n",
    "\n",
    "    # Handle categorical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_cols_processed = process_categorical_cols(X_train, categorical_cols, categorical_preprocessing_methods, feature_selection_methods, models, y_train)\n",
    "\n",
    "    # Concatenate the processed numerical and categorical columns\n",
    "    cols = (numerical_cols_processed or []) + (categorical_cols_processed or [])\n",
    "\n",
    "\n",
    "    preprocessing_techniques = []\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessing_techniques.append((f'num_preprocess_{col}', numerical_preprocessing_methods[preprocess_name], [col]))\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessing_techniques.append((f'cat_preprocess_{col}', categorical_preprocessing_methods[preprocess_name], [col]))\n",
    "            if feature_selection_name is not None:\n",
    "                preprocessing_techniques.append((f'feature_selection_{col}', feature_selection_methods[feature_selection_name], [col]))\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X, y = split_data(data, target_variable)\n",
    "\n",
    "    # Convert X_train and X_test to pandas DataFrames if they are not already in that format\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    # Convert column names to strings\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "    # Apply preprocessing steps to training data\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "            X_train[[col]] = preprocessor.fit_transform(X_train[[col]], y_train)\n",
    "\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "            X_train = preprocessor.fit_transform(X_train, y_train)\n",
    "            X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "            if feature_selection_name is not None:\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                X_train = feature_selector.fit_transform(X_train, y_train)\n",
    "                X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "\n",
    "    # Apply preprocessing steps to testing data\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "            preprocessor.fit(X_train[[col]])  # Fit on training data\n",
    "            X_test[[col]] = preprocessor.transform(X_test[[col]])\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "            X_test = preprocessor.transform(X_test)\n",
    "            X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "            if feature_selection_name is not None:\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                X_test = feature_selector.transform(X_test)\n",
    "                X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "\n",
    "    results_df_sorted, pipeline_counts = perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques)\n",
    "\n",
    "    excel_file_path = 'results_sorted.xlsx'\n",
    "    results_df_sorted.to_excel(excel_file_path, index=False)\n",
    "\n",
    "    total_pipelines = sum(count for count, _, _ in pipeline_counts)\n",
    "    total_preprocessing_steps = sum(len(steps) for _, steps, _ in pipeline_counts)\n",
    "\n",
    "    logger.info(f\"Total Models: {len(models)}\")\n",
    "    logger.info(f\"Total Pipelines: {total_pipelines}\")\n",
    "    logger.info(f\"Total Preprocessing Methods: {len(preprocessing_techniques)}\")\n",
    "    logger.info(f\"Total Preprocessing Steps: {total_preprocessing_steps}\")\n",
    "\n",
    "    best_train_result = results_df_sorted.iloc[0]\n",
    "    logger.info(\"Best Train Result:\")\n",
    "    logger.info(best_train_result)\n",
    "\n",
    "    best_test_result = results_df_sorted.sort_values(by='Test Score', ascending=False).iloc[0]\n",
    "    logger.info(\"Best Test Result:\")\n",
    "    logger.info(best_test_result)\n",
    "\n",
    "    logger.info(f\"Sorted Results saved to: {excel_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

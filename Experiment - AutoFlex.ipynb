{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Update file / folder directories as per your configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create a directory to save all datasets\n",
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "\n",
    "# Classification datasets from sklearn\n",
    "classification_datasets = {\n",
    "    'wine': datasets.load_wine(),\n",
    "    'breast_cancer': datasets.load_breast_cancer(),\n",
    "    'iris': datasets.load_iris(),\n",
    "    'digits': datasets.load_digits(),\n",
    "    'linnerud': datasets.load_linnerud(),\n",
    "    # Additional datasets from openml by name\n",
    "    'adult': datasets.fetch_openml(name='adult'),\n",
    "    'australian': datasets.fetch_openml(name='australian'),\n",
    "    'bank-marketing': datasets.fetch_openml(name='bank-marketing'),\n",
    "    'car': datasets.fetch_openml(name='car'),\n",
    "    'tic-tac-toe': datasets.fetch_openml(name='tic-tac-toe'),\n",
    "}\n",
    "\n",
    "for dataset_name, dataset in classification_datasets.items():\n",
    "    df = pd.DataFrame(data=dataset.data)\n",
    "    \n",
    "    if len(dataset.target.shape) > 1:\n",
    "        for i in range(dataset.target.shape[1]):\n",
    "            df[f'target_{i}'] = dataset.target[:, i]\n",
    "    else:\n",
    "        df['target'] = dataset.target\n",
    "\n",
    "    df.to_csv(f'datasets/{dataset_name}.csv', index=False)\n",
    "\n",
    "# Regression datasets from sklearn\n",
    "regression_datasets = {\n",
    "    'boston': datasets.load_boston(),\n",
    "    'diabetes': datasets.load_diabetes(),\n",
    "    'linnerud': datasets.load_linnerud(),\n",
    "    'california_housing': datasets.fetch_california_housing(),\n",
    "    # Additional datasets from openml by name\n",
    "    'boston': datasets.fetch_openml(name='boston'),\n",
    "    'diabetes': datasets.fetch_openml(name='diabetes'),\n",
    "    'house_8L': datasets.fetch_openml(name='house_8L'),\n",
    "    'cpu': datasets.fetch_openml(name='cpu'),\n",
    "    'qsar-biodeg': datasets.fetch_openml(name='qsar-biodeg'),\n",
    "    'bike_sharing': datasets.fetch_openml(name='Bike_Sharing_Demand')\n",
    "}\n",
    "\n",
    "for dataset_name, dataset in regression_datasets.items():\n",
    "    df = pd.DataFrame(data=dataset.data)\n",
    "    \n",
    "    if len(dataset.target.shape) > 1:\n",
    "        for i in range(dataset.target.shape[1]):\n",
    "            df[f'target_{i}'] = dataset.target[:, i]\n",
    "    else:\n",
    "        df['target'] = dataset.target\n",
    "\n",
    "    df.to_csv(f'datasets/{dataset_name}_regression.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment - AutoFlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dataset from {file_path}...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    logger.info(\"Dataset loaded.\")\n",
    "    return data\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset.\n",
    "    \"\"\"\n",
    "    logger.info(\"Handling missing values...\")\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Fill missing values with the mean for numerical columns\n",
    "    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())\n",
    "\n",
    "    # Fill missing values with \"Unknown\" for categorical columns\n",
    "    data[categorical_cols] = data[categorical_cols].fillna('Unknown')\n",
    "\n",
    "    logger.info(\"Missing values handled.\")\n",
    "    return data\n",
    "\n",
    "def split_data(data, target_variable):\n",
    "    \"\"\"\n",
    "    Split the data into features (X) and target variable (y).\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting the data into features and target variable...\")\n",
    "    X = data.drop(columns=[target_variable])\n",
    "    y = data[target_variable]\n",
    "    logger.info(\"Data split complete.\")\n",
    "    return X, y\n",
    "\n",
    "def preprocess_numerical_col(col, preprocessing_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a numerical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        for preprocess_name, preprocess_method in preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='passthrough')\n",
    "            for model_name, (model, model_params) in models.items():\n",
    "                pipeline = Pipeline([\n",
    "                    ('preprocessor', preprocessor),\n",
    "                    ('model', model)\n",
    "                ])\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                score = pipeline.score(X_train, y_train)\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score = score\n",
    "                    best_preprocess = preprocess_name\n",
    "        return col, best_preprocess\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, str(e)\n",
    "\n",
    "\n",
    "def preprocess_categorical_col(col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a categorical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        best_feature_selection = None  # Store the best feature selection method\n",
    "        for preprocess_name, preprocess_method in categorical_preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='drop')\n",
    "            for feature_selection_name, feature_selection_method in feature_selection_methods.items():\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_method, [col])])\n",
    "                for model_name, (model, model_params) in models.items():\n",
    "                    pipeline = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('feature_selection', feature_selector),\n",
    "                        ('model', model)\n",
    "                    ])\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_train, y_train)\n",
    "                    if best_score is None or score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preprocess = preprocess_name\n",
    "                        best_feature_selection = feature_selection_name\n",
    "        return col, best_preprocess, best_feature_selection\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, None, str(e)\n",
    "\n",
    "def process_numerical_cols(X_train, numerical_cols, preprocessing_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess numerical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing numerical columns...\")\n",
    "    numerical_cols_processed = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        numerical_cols_processed = parallel(delayed(preprocess_numerical_col)(col, preprocessing_methods, X_train, y_train, models) for col in tqdm(numerical_cols, desc=\"Numerical Columns Preprocessing\"))\n",
    "    logger.info(\"Numerical columns processing complete.\")\n",
    "    return numerical_cols_processed\n",
    "\n",
    "def process_categorical_cols(X_train, categorical_cols, preprocessing_methods, feature_selection_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess categorical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing categorical columns...\")\n",
    "    categorical_cols_processed = []\n",
    "    categorical_cols_processed_temp = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        categorical_cols_processed = parallel(delayed(preprocess_categorical_col)(\n",
    "            col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models\n",
    "        ) for col in tqdm(categorical_cols, desc=\"Categorical Columns Preprocessing\"))\n",
    "\n",
    "    # Add the processed categorical columns to the list\n",
    "    categorical_cols_processed.extend(cols for cols in categorical_cols_processed_temp if cols not in categorical_cols_processed)\n",
    "\n",
    "    return categorical_cols_processed\n",
    "\n",
    "\n",
    "\n",
    "def perform_grid_search(model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                        preprocessing_techniques):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation to find the best hyperparameters and preprocessing techniques.\n",
    "    \"\"\"\n",
    "    pipeline_count = 0\n",
    "    preprocessing_steps = []\n",
    "\n",
    "    if len(cols) > 0:\n",
    "        pipeline_count += 1\n",
    "        preprocessor = ColumnTransformer(preprocessing_techniques, remainder='passthrough')\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        param_grid = {f'model__{param_name}': param_range for param_name, param_range in model_params.items()}\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=KFold(n_splits=5))\n",
    "                grid_search.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Grid search failed for {model_name}: {e}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "        best_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        pipeline_score = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=KFold(n_splits=5)).mean()\n",
    "\n",
    "        if task_type == \"Regression\":\n",
    "            y_pred = grid_search.predict(X_train)\n",
    "            r2 = r2_score(y_train, y_pred)\n",
    "            test_score = grid_search.score(X_test, y_test)\n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Preprocessing': cols,\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Score': best_score,\n",
    "                'Pipeline Score': pipeline_score,\n",
    "                'Accuracy': r2,\n",
    "                'Test Score': test_score,\n",
    "                'Execution Time (s)': end_time - start_time\n",
    "            }\n",
    "        else:\n",
    "            y_pred = grid_search.predict(X_train)\n",
    "            accuracy = accuracy_score(y_train, y_pred)\n",
    "            test_score = grid_search.score(X_test, y_test)\n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Preprocessing': cols,\n",
    "                'Best Parameters': best_params,\n",
    "                'Best Score': best_score,\n",
    "                'Pipeline Score': pipeline_score,\n",
    "                'Accuracy': accuracy,\n",
    "                'Test Score': test_score,\n",
    "                'Execution Time (s)': end_time - start_time\n",
    "            }\n",
    "\n",
    "        preprocessing_steps = [f\"{step[0]}: {step[1]}\" for step in cols]\n",
    "\n",
    "        return result, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "    else:\n",
    "        return {}, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "\n",
    "def perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation in parallel for each model and column combination.\n",
    "    \"\"\"\n",
    "    pipeline_counts = []\n",
    "    preprocessing_techniques_str = []\n",
    "    results = []\n",
    "\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        total_pipelines_count = len(cols) * len(models)\n",
    "        pipeline_bar = tqdm(total=total_pipelines_count, desc='Optimization Progress', leave=False)\n",
    "        for model_name, (model, model_params) in models.items():\n",
    "            try:\n",
    "                processed_results = parallel(delayed(perform_grid_search)(\n",
    "                    model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                    preprocessing_techniques\n",
    "                ) for cols in tqdm(cols, desc=f\"Model: {model_name}\", total=len(cols), file=sys.stdout))\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Grid search parallel failed for {model_name}: {e}\")\n",
    "\n",
    "            results.extend([result for result, _, _, _ in processed_results])\n",
    "\n",
    "            pipeline_counts.extend([(count, steps, techniques) for _, count, steps, techniques in processed_results])\n",
    "\n",
    "            pipeline_bar.update(len(cols))\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            pipeline_bar.set_postfix({'Pipelines Generated': f\"{sum(count for count, _, _ in pipeline_counts)}/{total_pipelines_count}\"})\n",
    "\n",
    "    preprocessing_techniques_str = '\\n'.join([f\"{step[0]}: {step[1]}\" for step in preprocessing_techniques])\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    results_df['Preprocessing Techniques'] = preprocessing_techniques_str\n",
    "\n",
    "    results_df_sorted = results_df.sort_values(by='Best Score', ascending=False)\n",
    "\n",
    "    return results_df_sorted, pipeline_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Load your dataset\n",
    "    data = load_dataset('d.csv')\n",
    "\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "\n",
    "    # Assume the target variable column name is 'target'\n",
    "    target_variable = 'target'\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X, y = split_data(data, target_variable)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Calculate the unique percentage of the target variable\n",
    "    unique_percentage = y.nunique() / y.shape[0]\n",
    "\n",
    "    # Determine the target variable class based on the unique percentage\n",
    "    if unique_percentage <= 0.05:\n",
    "        target_variable_class = \"Binary\"\n",
    "    elif unique_percentage <= 0.1:\n",
    "        target_variable_class = \"Multi-class\"\n",
    "    else:\n",
    "        target_variable_class = \"Regression\"\n",
    "\n",
    "    # Print the target variable class\n",
    "    logger.info(\"Target Variable Class: %s\", target_variable_class)\n",
    "\n",
    "    # Define task type based on the target variable class\n",
    "    if target_variable_class == \"Binary\":\n",
    "        # Binary classification task\n",
    "        task_type = \"Binary Classification\"\n",
    "        models = {\n",
    "            'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Classifier': (MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "            'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "    elif target_variable_class == \"Multi-class\":\n",
    "        # Multi-class classification task\n",
    "        task_type = \"Multi-class Classification\"\n",
    "        models = {\n",
    "            'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Classifier': (MLPClassifier(max_iter=10000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "            'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "    else:\n",
    "        # Regression task\n",
    "        task_type = \"Regression\"\n",
    "        models = {\n",
    "            'Decision Tree Regressor': (DecisionTreeRegressor(), {'max_depth': [None, 3, 5, 10]}),\n",
    "            'Gradient Boosting Regressor': (GradientBoostingRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Random Forest Regressor': (RandomForestRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "            'Neural Network Regressor': (MLPRegressor(), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "            'Linear Regression': (LinearRegression(), {'normalize': [True, False]}),\n",
    "            'KNN Regressor': (KNeighborsRegressor(), {'n_neighbors': [3, 5, 7]}),\n",
    "        }\n",
    "\n",
    "    # Define your preprocessing techniques\n",
    "    numerical_preprocessing_methods = {\n",
    "        'SimpleImputer': SimpleImputer(),\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'PolynomialFeatures': PolynomialFeatures(),\n",
    "        'PCA': PCA()\n",
    "    }\n",
    "\n",
    "    categorical_preprocessing_methods = {\n",
    "        'OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
    "        'LabelEncoder': LabelEncoder(),\n",
    "    }\n",
    "\n",
    "    feature_selection_methods = {\n",
    "        'SelectKBest_f_regression': SelectKBest(f_regression),\n",
    "        'SelectKBest_chi2': SelectKBest(chi2)\n",
    "    }\n",
    "\n",
    "    # Handle numerical columns\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    numerical_cols_processed = process_numerical_cols(X_train, numerical_cols, numerical_preprocessing_methods, models, y_train)\n",
    "\n",
    "\n",
    "    # Handle categorical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_cols_processed = process_categorical_cols(X_train, categorical_cols, categorical_preprocessing_methods, feature_selection_methods, models, y_train)\n",
    "\n",
    "    # Concatenate the processed numerical and categorical columns\n",
    "    cols = (numerical_cols_processed or []) + (categorical_cols_processed or [])\n",
    "\n",
    "\n",
    "    preprocessing_techniques = []\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessing_techniques.append((f'num_preprocess_{col}', numerical_preprocessing_methods[preprocess_name], [col]))\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessing_techniques.append((f'cat_preprocess_{col}', categorical_preprocessing_methods[preprocess_name], [col]))\n",
    "            if feature_selection_name is not None:\n",
    "                preprocessing_techniques.append((f'feature_selection_{col}', feature_selection_methods[feature_selection_name], [col]))\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X, y = split_data(data, target_variable)\n",
    "\n",
    "    # Convert X_train and X_test to pandas DataFrames if they are not already in that format\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    # Convert column names to strings\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "    # Apply preprocessing steps to training data\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "            X_train[[col]] = preprocessor.fit_transform(X_train[[col]], y_train)\n",
    "\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "            X_train = preprocessor.fit_transform(X_train, y_train)\n",
    "            X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "            if feature_selection_name is not None:\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                X_train = feature_selector.fit_transform(X_train, y_train)\n",
    "                X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "\n",
    "    # Apply preprocessing steps to testing data\n",
    "    for col, preprocess_name in numerical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "            preprocessor.fit(X_train[[col]])  # Fit on training data\n",
    "            X_test[[col]] = preprocessor.transform(X_test[[col]])\n",
    "\n",
    "    for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "        if preprocess_name is not None:\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "            X_test = preprocessor.transform(X_test)\n",
    "            X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "            if feature_selection_name is not None:\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                X_test = feature_selector.transform(X_test)\n",
    "                X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "\n",
    "    results_df_sorted, pipeline_counts = perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques)\n",
    "\n",
    "    excel_file_path = 'results_sorted.xlsx'\n",
    "    results_df_sorted.to_excel(excel_file_path, index=False)\n",
    "\n",
    "    total_pipelines = sum(count for count, _, _ in pipeline_counts)\n",
    "    total_preprocessing_steps = sum(len(steps) for _, steps, _ in pipeline_counts)\n",
    "\n",
    "    logger.info(f\"Total Models: {len(models)}\")\n",
    "    logger.info(f\"Total Pipelines: {total_pipelines}\")\n",
    "    logger.info(f\"Total Preprocessing Methods: {len(preprocessing_techniques)}\")\n",
    "    logger.info(f\"Total Preprocessing Steps: {total_preprocessing_steps}\")\n",
    "\n",
    "    best_train_result = results_df_sorted.iloc[0]\n",
    "    logger.info(\"Best Train Result:\")\n",
    "    logger.info(best_train_result)\n",
    "\n",
    "    best_test_result = results_df_sorted.sort_values(by='Test Score', ascending=False).iloc[0]\n",
    "    logger.info(\"Best Test Result:\")\n",
    "    logger.info(best_test_result)\n",
    "\n",
    "    logger.info(f\"Sorted Results saved to: {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dataset from {file_path}...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    logger.info(\"Dataset loaded.\")\n",
    "    return data\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset.\n",
    "    \"\"\"\n",
    "    logger.info(\"Handling missing values...\")\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Fill missing values with the mean for numerical columns\n",
    "    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())\n",
    "\n",
    "    # Fill missing values with \"Unknown\" for categorical columns\n",
    "    data[categorical_cols] = data[categorical_cols].fillna('Unknown')\n",
    "\n",
    "    logger.info(\"Missing values handled.\")\n",
    "    return data\n",
    "\n",
    "def split_data(data, target_variable):\n",
    "    \"\"\"\n",
    "    Split the data into features (X) and target variable (y).\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting the data into features and target variable...\")\n",
    "    X = data.drop(columns=[target_variable])\n",
    "    y = data[target_variable]\n",
    "    logger.info(\"Data split complete.\")\n",
    "    return X, y\n",
    "\n",
    "def preprocess_numerical_col(col, preprocessing_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a numerical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        for preprocess_name, preprocess_method in preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='passthrough')\n",
    "            for model_name, (model, model_params) in models.items():\n",
    "                pipeline = Pipeline([\n",
    "                    ('preprocessor', preprocessor),\n",
    "                    ('model', model)\n",
    "                ])\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                score = pipeline.score(X_train, y_train)\n",
    "                if best_score is None or score > best_score:\n",
    "                    best_score = score\n",
    "                    best_preprocess = preprocess_name\n",
    "        return col, best_preprocess\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, str(e)\n",
    "\n",
    "\n",
    "def preprocess_categorical_col(col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models):\n",
    "    \"\"\"\n",
    "    Preprocess a categorical column and find the best preprocessing technique.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        best_preprocess = None\n",
    "        best_score = None\n",
    "        best_feature_selection = None  # Store the best feature selection method\n",
    "        for preprocess_name, preprocess_method in categorical_preprocessing_methods.items():\n",
    "            preprocessor = ColumnTransformer([(preprocess_name, preprocess_method, [col])], remainder='drop')\n",
    "            for feature_selection_name, feature_selection_method in feature_selection_methods.items():\n",
    "                feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_method, [col])])\n",
    "                for model_name, (model, model_params) in models.items():\n",
    "                    pipeline = Pipeline([\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('feature_selection', feature_selector),\n",
    "                        ('model', model)\n",
    "                    ])\n",
    "                    pipeline.fit(X_train, y_train)\n",
    "                    score = pipeline.score(X_train, y_train)\n",
    "                    if best_score is None or score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preprocess = preprocess_name\n",
    "                        best_feature_selection = feature_selection_name\n",
    "        return col, best_preprocess, best_feature_selection\n",
    "    except (ValueError, TypeError) as e:\n",
    "        return col, None, None, str(e)\n",
    "\n",
    "def process_numerical_cols(X_train, numerical_cols, preprocessing_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess numerical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing numerical columns...\")\n",
    "    numerical_cols_processed = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        numerical_cols_processed = parallel(delayed(preprocess_numerical_col)(col, preprocessing_methods, X_train, y_train, models) for col in tqdm(numerical_cols, desc=\"Numerical Columns Preprocessing\"))\n",
    "    logger.info(\"Numerical columns processing complete.\")\n",
    "    return numerical_cols_processed\n",
    "\n",
    "def process_categorical_cols(X_train, categorical_cols, preprocessing_methods, feature_selection_methods, models, y_train):\n",
    "    \"\"\"\n",
    "    Preprocess categorical columns and find the best preprocessing techniques for each column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing categorical columns...\")\n",
    "    categorical_cols_processed = []\n",
    "    categorical_cols_processed_temp = []\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        categorical_cols_processed = parallel(delayed(preprocess_categorical_col)(\n",
    "            col, categorical_preprocessing_methods, feature_selection_methods, X_train, y_train, models\n",
    "        ) for col in tqdm(categorical_cols, desc=\"Categorical Columns Preprocessing\"))\n",
    "\n",
    "    # Add the processed categorical columns to the list\n",
    "    categorical_cols_processed.extend(cols for cols in categorical_cols_processed_temp if cols not in categorical_cols_processed)\n",
    "\n",
    "    return categorical_cols_processed\n",
    "\n",
    "\n",
    "\n",
    "def perform_grid_search(model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                        preprocessing_techniques):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        Perform grid search with cross-validation to find the best hyperparameters and preprocessing techniques.\n",
    "        \"\"\"\n",
    "        pipeline_count = 0\n",
    "        preprocessing_steps = []\n",
    "\n",
    "        if len(cols) > 0:\n",
    "            pipeline_count += 1\n",
    "            preprocessor = ColumnTransformer(preprocessing_techniques, remainder='passthrough')\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            param_grid = {f'model__{param_name}': param_range for param_name, param_range in model_params.items()}\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "                start_time = time.time()\n",
    "\n",
    "                try:\n",
    "                    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=KFold(n_splits=5))\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Grid search failed for {model_name}: {e}\")\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "            best_score = grid_search.best_score_\n",
    "            best_params = grid_search.best_params_\n",
    "            pipeline_score = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=KFold(n_splits=5)).mean()\n",
    "\n",
    "            if task_type == \"Regression\":\n",
    "                y_pred = grid_search.predict(X_train)\n",
    "                r2 = r2_score(y_train, y_pred)\n",
    "                test_score = grid_search.score(X_test, y_test)\n",
    "                result = {\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': cols,\n",
    "                    'Best Parameters': best_params,\n",
    "                    'Best Score': best_score,\n",
    "                    'Pipeline Score': pipeline_score,\n",
    "                    'Accuracy': r2,\n",
    "                    'Test Score': test_score,\n",
    "                    'Execution Time (s)': end_time - start_time\n",
    "                }\n",
    "            else:\n",
    "                y_pred = grid_search.predict(X_train)\n",
    "                accuracy = accuracy_score(y_train, y_pred)\n",
    "                test_score = grid_search.score(X_test, y_test)\n",
    "                result = {\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': cols,\n",
    "                    'Best Parameters': best_params,\n",
    "                    'Best Score': best_score,\n",
    "                    'Pipeline Score': pipeline_score,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Test Score': test_score,\n",
    "                    'Execution Time (s)': end_time - start_time\n",
    "                }\n",
    "\n",
    "            preprocessing_steps = [f\"{step[0]}: {step[1]}\" for step in cols]\n",
    "\n",
    "            return result, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "        else:\n",
    "            return {}, pipeline_count, preprocessing_steps, preprocessing_techniques\n",
    "        \n",
    "    except (ValueError, TypeError, IndexError) as e:\n",
    "        # Log the error and raise a more informative exception\n",
    "        logger.error(f\"Error occurred during grid search for {model_name}: {e}\")\n",
    "        logger.error(f\"Cols: {cols}\")\n",
    "        logger.error(f\"Preprocessing Techniques: {preprocessing_techniques}\")\n",
    "        raise Exception(f\"Grid search failed for {model_name}: {e}\")\n",
    "\n",
    "def perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation in parallel for each model and column combination.\n",
    "    \"\"\"\n",
    "    pipeline_counts = []\n",
    "    preprocessing_techniques_str = []\n",
    "    results = []\n",
    "\n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        total_pipelines_count = len(cols) * len(models)\n",
    "        pipeline_bar = tqdm(total=total_pipelines_count, desc='Optimization Progress', leave=False)\n",
    "        for model_name, (model, model_params) in models.items():\n",
    "            try:\n",
    "                processed_results = parallel(delayed(perform_grid_search)(\n",
    "                    model_name, model, model_params, cols, X_train, y_train, X_test, y_test, task_type,\n",
    "                    preprocessing_techniques\n",
    "                ) for cols in tqdm(cols, desc=f\"Model: {model_name}\", total=len(cols), file=sys.stdout))\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Grid search parallel failed for {model_name}: {e}\")\n",
    "\n",
    "            results.extend([result for result, _, _, _ in processed_results])\n",
    "\n",
    "            pipeline_counts.extend([(count, steps, techniques) for _, count, steps, techniques in processed_results])\n",
    "\n",
    "            pipeline_bar.update(len(cols))\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            pipeline_bar.set_postfix({'Pipelines Generated': f\"{sum(count for count, _, _ in pipeline_counts)}/{total_pipelines_count}\"})\n",
    "\n",
    "    preprocessing_techniques_str = '\\n'.join([f\"{step[0]}: {step[1]}\" for step in preprocessing_techniques])\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    results_df['Preprocessing Techniques'] = preprocessing_techniques_str\n",
    "    results_df_sorted = results_df.sort_values(by='Best Score', ascending=False)\n",
    "\n",
    "    return results_df_sorted, pipeline_counts\n",
    "\n",
    "# List of percentages for record selection\n",
    "percentages = [0.25, 0.5, 1.0]\n",
    "\n",
    "# Dictionary of notations for each percentage\n",
    "percent_notations = {\n",
    "    0.25: \"25p\",\n",
    "    0.5: \"50p\",\n",
    "    1.0: \"100p\"\n",
    "}\n",
    "\n",
    "# Assume the target variable column name is 'target'\n",
    "target_variable = 'target'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    folder_path = 'datasets/INT'\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "        # Load the dataset\n",
    "        data = load_dataset(file_path)\n",
    "\n",
    "        # Handle missing values\n",
    "        data = handle_missing_values(data)\n",
    "        \n",
    "        for percentage in percentages:\n",
    "            # Determine the number of records based on the percentage\n",
    "            num_records = int(data.shape[0] * percentage)\n",
    "\n",
    "            # Sample the specified number of records\n",
    "            sampled_data = data.sample(n=num_records, random_state=42)\n",
    "\n",
    "            # Handle missing values\n",
    "            sampled_data = handle_missing_values(sampled_data)\n",
    "            \n",
    "            # Split the data into features (X) and target variable (y)\n",
    "            X, y = split_data(sampled_data, target_variable)\n",
    "\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Calculate the unique percentage of the target variable\n",
    "            unique_percentage = y.nunique() / y.shape[0]\n",
    "\n",
    "            # Determine the target variable class based on the unique percentage\n",
    "            if unique_percentage <= 0.05:\n",
    "                target_variable_class = \"Binary\"\n",
    "            elif unique_percentage <= 0.1:\n",
    "                target_variable_class = \"Multi-class\"\n",
    "            else:\n",
    "                target_variable_class = \"Regression\"\n",
    "\n",
    "            # Print the target variable class\n",
    "            logger.info(\"Target Variable Class: %s\", target_variable_class)\n",
    "\n",
    "            # Define task type based on the target variable class\n",
    "            if target_variable_class == \"Binary\":\n",
    "                # Binary classification task\n",
    "                task_type = \"Binary Classification\"\n",
    "                models = {\n",
    "                    'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "                    'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Neural Network Classifier': (MLPClassifier(max_iter=1000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "                    'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "                    'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "                }\n",
    "            elif target_variable_class == \"Multi-class\":\n",
    "                # Multi-class classification task\n",
    "                task_type = \"Multi-class Classification\"\n",
    "                models = {\n",
    "                    'Decision Tree Classifier': (DecisionTreeClassifier(), {'max_depth': [None, 3, 5, 10]}),\n",
    "                    'Gradient Boosting Classifier': (GradientBoostingClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Random Forest Classifier': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Neural Network Classifier': (MLPClassifier(max_iter=10000), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "                    'Logistic Regression': (LogisticRegression(max_iter=10000), {'penalty': ['l2'], 'C': [0.1, 1, 10]}),\n",
    "                    'KNN Classifier': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "                }\n",
    "            else:\n",
    "                # Regression task\n",
    "                task_type = \"Regression\"\n",
    "                models = {\n",
    "                    'Decision Tree Regressor': (DecisionTreeRegressor(), {'max_depth': [None, 3, 5, 10]}),\n",
    "                    'Gradient Boosting Regressor': (GradientBoostingRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Random Forest Regressor': (RandomForestRegressor(), {'n_estimators': [50, 100, 200]}),\n",
    "                    'Neural Network Regressor': (MLPRegressor(), {'hidden_layer_sizes': [(50,), (100,), (100, 50)]}),\n",
    "                    'Linear Regression': (LinearRegression(), {'copy_X': [True, False]}),\n",
    "                    'KNN Regressor': (KNeighborsRegressor(), {'n_neighbors': [3, 5, 7]}),\n",
    "                }\n",
    "\n",
    "            # Define your preprocessing techniques\n",
    "            numerical_preprocessing_methods = {\n",
    "                #'SimpleImputer': SimpleImputer(),\n",
    "                'StandardScaler': StandardScaler(),\n",
    "                'RobustScaler': RobustScaler(),\n",
    "                'MinMaxScaler': MinMaxScaler(),\n",
    "                'PolynomialFeatures': PolynomialFeatures(),\n",
    "                'PCA': PCA()\n",
    "            }\n",
    "\n",
    "            categorical_preprocessing_methods = {\n",
    "                'OneHotEncoder': OneHotEncoder(handle_unknown='ignore'),\n",
    "                'LabelEncoder': LabelEncoder(),\n",
    "            }\n",
    "\n",
    "            feature_selection_methods = {\n",
    "                'SelectKBest_f_regression': SelectKBest(f_regression),\n",
    "                'SelectKBest_chi2': SelectKBest(chi2)\n",
    "            }\n",
    "\n",
    "            # Handle numerical columns\n",
    "            numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "            numerical_cols_processed = process_numerical_cols(X_train, numerical_cols, numerical_preprocessing_methods, models, y_train)\n",
    "\n",
    "\n",
    "            # Handle categorical columns\n",
    "            categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "            categorical_cols_processed = process_categorical_cols(X_train, categorical_cols, categorical_preprocessing_methods, feature_selection_methods, models, y_train)\n",
    "\n",
    "            # Concatenate the processed numerical and categorical columns\n",
    "            cols = (numerical_cols_processed or []) + (categorical_cols_processed or [])\n",
    "\n",
    "\n",
    "            preprocessing_techniques = []\n",
    "            for col, preprocess_name in numerical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessing_techniques.append((f'num_preprocess_{col}', numerical_preprocessing_methods[preprocess_name], [col]))\n",
    "\n",
    "            for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessing_techniques.append((f'cat_preprocess_{col}', categorical_preprocessing_methods[preprocess_name], [col]))\n",
    "                    if feature_selection_name is not None:\n",
    "                        preprocessing_techniques.append((f'feature_selection_{col}', feature_selection_methods[feature_selection_name], [col]))\n",
    "\n",
    "            # Split the data into features (X) and target variable (y)\n",
    "            X, y = split_data(data, target_variable)\n",
    "\n",
    "            # Convert X_train and X_test to pandas DataFrames if they are not already in that format\n",
    "            X_train = pd.DataFrame(X_train)\n",
    "            X_test = pd.DataFrame(X_test)\n",
    "\n",
    "            # Convert column names to strings\n",
    "            X_train.columns = X_train.columns.astype(str)\n",
    "            X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "            # Apply preprocessing steps to training data\n",
    "            for col, preprocess_name in numerical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "                    X_train[[col]] = preprocessor.fit_transform(X_train[[col]], y_train)\n",
    "\n",
    "\n",
    "            for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "                    X_train = preprocessor.fit_transform(X_train, y_train)\n",
    "                    X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "                    if feature_selection_name is not None:\n",
    "                        feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                        X_train = feature_selector.fit_transform(X_train, y_train)\n",
    "                        X_train = pd.DataFrame(X_train)  # Convert back to pandas DataFrame\n",
    "\n",
    "            # Apply preprocessing steps to testing data\n",
    "            for col, preprocess_name in numerical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessor = ColumnTransformer([(preprocess_name, numerical_preprocessing_methods[preprocess_name], [col])], remainder='passthrough')\n",
    "                    preprocessor.fit(X_train[[col]])  # Fit on training data\n",
    "                    X_test[[col]] = preprocessor.transform(X_test[[col]])\n",
    "\n",
    "            for col, preprocess_name, feature_selection_name in categorical_cols_processed:\n",
    "                if preprocess_name is not None:\n",
    "                    preprocessor = ColumnTransformer([(preprocess_name, categorical_preprocessing_methods[preprocess_name], [col])], remainder='drop')\n",
    "                    X_test = preprocessor.transform(X_test)\n",
    "                    X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "                    if feature_selection_name is not None:\n",
    "                        feature_selector = ColumnTransformer([(feature_selection_name, feature_selection_methods[feature_selection_name], [col])])\n",
    "                        X_test = feature_selector.transform(X_test)\n",
    "                        X_test = pd.DataFrame(X_test)  # Convert back to pandas DataFrame\n",
    "\n",
    "            results_df_sorted, pipeline_counts = perform_grid_search_parallel(models, cols, X_train, y_train, X_test, y_test, task_type, preprocessing_techniques)\n",
    "\n",
    "            excel_file_name = f\"results/{csv_file.split('.')[0]}_{percent_notations[percentage]}_results_sorted.xlsx\"\n",
    "            excel_file_path = excel_file_name\n",
    "\n",
    "            #Add the \"File Name\" column to the DataFrame and move it to the first position\n",
    "            results_df_sorted.insert(0, \"File Name\", csv_file.split('.')[0])    \n",
    "\n",
    "            # Create the directory if it doesn't exist\n",
    "            if not os.path.exists(\"results\"):\n",
    "                os.makedirs(\"results\")\n",
    "\n",
    "            if os.path.exists(excel_file_path):\n",
    "                os.remove(excel_file_path)\n",
    "            results_df_sorted.to_excel(excel_file_path, index=False, sheet_name='Results')\n",
    "\n",
    "            total_pipelines = sum(count for count, _, _ in pipeline_counts)\n",
    "            total_preprocessing_steps = sum(len(steps) for _, steps, _ in pipeline_counts)\n",
    "\n",
    "            logger.info(f\"Total Models: {len(models)}\")\n",
    "            logger.info(f\"Total Pipelines: {total_pipelines}\")\n",
    "            logger.info(f\"Total Preprocessing Methods: {len(preprocessing_techniques)}\")\n",
    "            logger.info(f\"Total Preprocessing Steps: {total_preprocessing_steps}\")\n",
    "\n",
    "            best_train_result = results_df_sorted.iloc[0]\n",
    "            logger.info(\"Best Train Result:\")\n",
    "            logger.info(best_train_result)\n",
    "\n",
    "            best_test_result = results_df_sorted.sort_values(by='Test Score', ascending=False).iloc[0]\n",
    "            logger.info(\"Best Test Result:\")\n",
    "            logger.info(best_test_result)\n",
    "\n",
    "            # Print the path of the saved Excel file\n",
    "            logger.info(f\"Sorted Results saved to: {excel_file_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Define the folder path containing the datasets\n",
    "folder_path = 'datasets/INT'\n",
    "\n",
    "# Get the list of CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Dataset', 'Task Type', 'Observation Percentage', 'TPOT Score', 'Auto-sklearn Score', 'H2O AutoML Score',\n",
    "                                   'TPOT Execution Time', 'Auto-sklearn Execution Time', 'H2O AutoML Execution Time',\n",
    "                                   'TPOT Best Model', 'Auto-sklearn Best Model', 'H2O AutoML Best Model',\n",
    "                                   'TPOT Pipeline Scores', 'Auto-sklearn Pipeline Scores', 'H2O AutoML Pipeline Scores'])\n",
    "\n",
    "# Iterate over each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    \n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    print(file_path)\n",
    "\n",
    "    # Assume the target variable column name is 'target'\n",
    "    target_variable = 'target'\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = data.drop(columns=[target_variable])\n",
    "    y = data[target_variable]\n",
    "\n",
    "    # Define the task type based on the target variable class\n",
    "    unique_percentage = y.nunique() / y.shape[0]\n",
    "    if unique_percentage <= 0.05:\n",
    "        task_type = \"classification\"\n",
    "    elif unique_percentage <= 0.1:\n",
    "        task_type = \"multiclassification\"\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "\n",
    "    print(task_type)\n",
    "\n",
    "    # Iterate over different observation percentages\n",
    "    for percentage in [0.25, 0.5, 1.0]:\n",
    "        # Determine the number of observations based on the percentage\n",
    "        num_observations = int(len(X) * percentage)\n",
    "\n",
    "        # Take a subset of the data based on the number of observations\n",
    "        X_subset = X.iloc[:num_observations]\n",
    "        y_subset = y.iloc[:num_observations]\n",
    "\n",
    "        # Split the subset data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "        # TPOT\n",
    "        tpot_start_time = time.time()\n",
    "        if task_type == \"classification\" or task_type == \"multiclassification\":\n",
    "            tpot = TPOTClassifier(generations=5, population_size=50, random_state=42, verbosity=2)\n",
    "        else:\n",
    "            tpot = TPOTRegressor(generations=5, population_size=50, random_state=42, verbosity=2)\n",
    "\n",
    "        tpot.fit(X_train, y_train)\n",
    "        tpot_execution_time = time.time() - tpot_start_time\n",
    "        tpot_score = tpot.score(X_test, y_test)\n",
    "        tpot_best_model = tpot.fitted_pipeline_\n",
    "        tpot_pipeline_scores = tpot.evaluated_individuals_\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        results_df = results_df.append({'Dataset': csv_file, 'Task Type': task_type, 'Observation Percentage': percentage,\n",
    "                                        'TPOT Score': tpot_score, \n",
    "\n",
    "                                        'TPOT Execution Time': tpot_execution_time,\n",
    "\n",
    "                                        'TPOT Best Model': tpot_best_model,\n",
    "\n",
    "                                        'TPOT Pipeline Scores': tpot_pipeline_scores},\n",
    "\n",
    "                                       ignore_index=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_file = 'tpot_results.xlsx'\n",
    "results_df.to_excel(results_file, index=False)\n",
    "print(\"Results saved to:\", results_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Split to Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Folder path containing the CSV files\n",
    "folder_path = \"datasets/INT\"\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read the CSV file\n",
    "        with open(file_path, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "\n",
    "        # Iterate over different observation percentages\n",
    "        for percentage in [0.25, 0.5, 1.0]:\n",
    "            # Determine the number of observations based on the percentage\n",
    "            num_observations = int(len(data) * percentage)\n",
    "\n",
    "            # Generate the new CSV file name\n",
    "            new_file_name = f\"{int(percentage * 100)}p_{file_name}\"\n",
    "\n",
    "            # Create a new CSV file in a separate folder\n",
    "            new_folder_path = \"new_folder_with_csvs\"\n",
    "            os.makedirs(new_folder_path, exist_ok=True)\n",
    "            new_file_path = os.path.join(new_folder_path, new_file_name)\n",
    "\n",
    "            # Write the data to the new CSV file\n",
    "            with open(new_file_path, mode='w', newline='') as new_file:\n",
    "                writer = csv.writer(new_file)\n",
    "                writer.writerows(data[:num_observations])\n",
    "\n",
    "            print(f\"New CSV file {new_file_name} created in the folder {new_folder_path}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import time\n",
    "\n",
    "def run_h2o_automl(csv_file):\n",
    "    h2o.init()\n",
    "    # Load the CSV file into H2O\n",
    "    data = h2o.import_file(csv_file)\n",
    "    # Set the target variable\n",
    "    target = 'target'\n",
    "\n",
    "    # Run H2O AutoML\n",
    "    aml = H2OAutoML(max_runtime_secs=300, max_runtime_secs_per_model=60)\n",
    "\n",
    "    aml.train(y=target, training_frame=data)\n",
    "\n",
    "    # Get the leaderboard\n",
    "    leaderboard = aml.leaderboard\n",
    "    leaderboard_df = leaderboard.as_data_frame()\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = aml.leader\n",
    "\n",
    "    # Get the pipeline scores\n",
    "    pipeline_scores = pd.DataFrame(best_model.scoring_history())\n",
    "\n",
    "    return leaderboard_df, best_model, pipeline_scores\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Dataset','leaderboard_df', 'H2O AutoML Execution Time', 'H2O AutoML Best Model', 'H2O AutoML Pipeline Scores'])\n",
    "\n",
    "\n",
    "folder_path = 'new_folder_with_csvs'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(folder_path, filename)\n",
    "        print(csv_file)\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        leaderboard_df, best_model, pipeline_scores = run_h2o_automl(csv_file)\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Append the results to the DataFrame\n",
    "        results_df = results_df.append({\n",
    "            'Dataset': csv_file,\n",
    "            'H2O leaderboard_df': leaderboard_df,\n",
    "            'H2O AutoML Execution Time': execution_time,\n",
    "            'H2O AutoML Best Model': best_model.model_id,\n",
    "            'H2O AutoML Pipeline Scores': pipeline_scores\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        h2o.shutdown()\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "output_file = 'h2o_excel_file2.xlsx'\n",
    "results_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correltion between Execution time and Model Accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the provided data\n",
    "data = {\n",
    "    'Subset ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "    'TPOT time': [11.31737, 18.6722, 104.2202, 55.56861, 136.1373, 18.39068, 62.11533, 78.67906, 77.77921, 112.801, 74.58719, 142.5261, 83.22899, 113.8769, 146.8045, 177.662, 98.19801, 90.08522, 110.0856, 65.36148, 120.7857, 316.6925, 23.63281, 367.1847, 82.60517, 1118.463, 700.4246, 1394.041, 858.111, 3765.884],\n",
    "    'TPOT score': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0.965517, 0.95614, 0.894737, 0.855072, 0.828571, 0.884058, 0.772727, 0.74359, 0.753247, 0.877358, 1, 0.862559, 0.988889, 0.977778, 0.983333, 0, 0, 0, 0],\n",
    "    'AutoFlex time': [0.234, 1.6476, 0.2208, 2.5347, 2.4179, 1.916, 1.5536, 1.5288, 0.8817, 2.3265, 1.7511, 2.9757, 3.1438, 1.5556, 4.1173, 3.8609, 3.6914, 2.9864, 3.5765, 0.5166, 0.5116, 17.08, 5.0032, 35.623, 10.107, 20.344, 7.1832, 31.097, 15.37, 61.619],\n",
    "    'AutoFlex score': [1, 1, 0.95, 1, 0.9857, 1, 0.7199, 0.9427, 0.9382, 0.7736, 0.704, 0.8706, 0.9909, 0.9802, 0.9868, 0.8913, 0.9053, 0.8877, 0.7688, 0.7843, 0.8081, 0.8788, 0.9048, 0.8874, 0.9526, 0.9805, 0.9791, 0.8041, 0.7803, 0.8052],\n",
    "    'H2O score': [0, None, 0.306, 0.52, 0.121, None, 0.391759, 0.22964, 0.277712, 0.47598, 0.32591, 0.73521, 0.33, 0.186, 0.281, 0.885, 0.914, 0.904, 0.1554, 0.163, 0.1757, None, None, None, 0.46, 0.2731, 0.424, 0.1604, 0.1459, 0.1809],\n",
    "    'H2O time': [299.4589, None, 300.4525, 307.3777, 299.2099, None, 295.9526, 306.3068, 299.7177, 298.5576, 299.0544, 306.3237, 299.0565, 301.3412, 299.1535, 299.331, 299.4442, 298.9338, 294.89, 298.8632, 298.4761, None, None, None, 300.2729, 305.1297, 301.2625, 307.6791, 307.1827, 307.7052]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation coefficients\n",
    "correlation_tpot = df['TPOT time'].corr(df['TPOT score'])\n",
    "correlation_autoflex = df['AutoFlex time'].corr(df['AutoFlex score'])\n",
    "correlation_h2o = df['H2O time'].corr(df['H2O score'])\n",
    "\n",
    "print(\"Correlation coefficient for TPOT: \", correlation_tpot)\n",
    "print(\"Correlation coefficient for AutoFlex: \", correlation_autoflex)\n",
    "print(\"Correlation coefficient for H2O AutoML: \", correlation_h2o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the folder containing the CSV files\n",
    "folder_path = \"datasets/finished\"\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Iterate through each CSV file and retrieve its shape\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    shape = df.shape\n",
    "    print(f\"{file}: {shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
